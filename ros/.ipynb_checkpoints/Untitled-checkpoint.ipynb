{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_path = glob.glob('./src/tl_detector/light_classification/traffic_light_classifier/test_img/*')\n",
    "X_test_list=[]\n",
    "RESIZE=32\n",
    "for im_path in X_test_path:\n",
    "    im = np.asarray(Image.open(im_path).resize((RESIZE,RESIZE)))\n",
    "    X_test_list.append(im)\n",
    "\n",
    "X_test= np.array(X_test_list)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver = tf.train.import_meta_graph('model_v10.ckpt.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lights = {0:'Red', 1:'Yellow', 2:'Green'}\n",
    "x = tf.get_default_graph().get_tensor_by_name('x:0')\n",
    "drop_out = tf.get_default_graph().get_tensor_by_name('drop_out:0')\n",
    "predict_op = tf.get_collection('predict_op') \n",
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('.'))\n",
    "    y_preds = session.run(predict_op, feed_dict={x:X_test, drop_out:1})[0]\n",
    "    i =0\n",
    "    for y in y_preds:\n",
    "        print(lights[y])\n",
    "        im = Image.open(X_test_path[i])\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        i+=1\n",
    "        print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.get_checkpoint_state('./src/tl_detector/light_classification/traffic_light_classifier/model_v11/')\n",
    "input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "# We precise the file fullname of our freezed graph\n",
    "absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "output_graph = absolute_model_folder + \"/frozen_model.pb\"\n",
    "\n",
    "# Before exporting our graph, we need to precise what is our output node\n",
    "# This is how TF decides what part of the Graph he has to keep and what part it can dump\n",
    "# NOTE: this variable is plural, because you can have multiple output nodes\n",
    "output_node_names = \"x, y, drop_out\"\n",
    "\n",
    "# We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "clear_devices = True\n",
    "\n",
    "# We import the meta graph and retrieve a Saver\n",
    "saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "# We retrieve the protobuf graph definition\n",
    "graph = tf.get_default_graph()\n",
    "input_graph_def = graph.as_graph_def()\n",
    "print(input_graph_def)\n",
    "# We start a session and restore the graph weights\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, input_checkpoint)\n",
    "\n",
    "#     # We use a built-in TF helper to export variables to constants\n",
    "#     output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "#         sess, # The session is used to retrieve the weights\n",
    "#         input_graph_def, # The graph_def is used to retrieve the nodes \n",
    "#         output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "#     ) \n",
    "\n",
    "#     # Finally we serialize and dump the output graph to the filesystem\n",
    "#     with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "#         f.write(output_graph_def.SerializeToString())\n",
    "#     print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph( graph_file, use_xla=False):\n",
    "    jit_level = 0\n",
    "    config = tf.ConfigProto()\n",
    "    if use_xla:\n",
    "        jit_level = tf.OptimizerOptions.ON_1\n",
    "        config.graph_options.optimizer_options.global_jit_level = jit_level\n",
    "\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        gd = tf.GraphDef()\n",
    "        with tf.gfile.Open(graph_file, 'rb') as f:\n",
    "            data = f.read()\n",
    "            gd.ParseFromString(data)\n",
    "        tf.import_graph_def(gd, name='')\n",
    "        ops = sess.graph.get_operations()\n",
    "        n_ops = len(ops)\n",
    "    return sess, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_tl, base_ops_tl = load_graph(\"./model_v10/frozen_model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=sess_tl.graph\n",
    "with tf.Session() as session:\n",
    "    drop_out = tf.get_default_graph().get_tensor_by_name('x')\n",
    "#     predict_op = tf.get_collection('y:0') \n",
    "    print(drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "with tf.gfile.FastGFile(\"./model_v10/\" + \"graph.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "frozen_graph_def = convert_variables_to_constants(sess, graph_def, [\"output\"])\n",
    "\n",
    "with tf.gfile.GFile(\"./model_v10/\" + \"frozen.pb\", \"wb\") as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34deac6e5b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrestore_op_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"save/restore_all\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfilename_tensor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"save/Const:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput_frozen_graph_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frozen_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0moutput_optimized_graph_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'optimized_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclear_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'model_v11'\n",
    "input_graph_path = MODEL_NAME+'.pbtxt'\n",
    "checkpoint_path = './src/tl_detector/light_classification/traffic_light_classifier/model_v11/model_v11.ckpt.data-00000-of-00001'\n",
    "input_saver_def_path = \"\"\n",
    "input_binary = False\n",
    "output_node_names = \"O\"\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "output_frozen_graph_name = 'frozen_'+MODEL_NAME+'.pb'\n",
    "output_optimized_graph_name = 'optimized_'+MODEL_NAME+'.pb'\n",
    "clear_devices = True\n",
    "\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                          input_binary, checkpoint_path, output_node_names,\n",
    "                          restore_op_name, filename_tensor_name,\n",
    "                          output_frozen_graph_name, clear_devices, \"\")\n",
    "\n",
    "\n",
    "\n",
    "# Optimize for inference\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open(output_frozen_graph_name, \"r\") as f:\n",
    "    data = f.read()\n",
    "    input_graph_def.ParseFromString(data)\n",
    "\n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "        input_graph_def,\n",
    "        [\"I\"], # an array of the input node(s)\n",
    "        [\"O\"], # an array of output nodes\n",
    "        tf.float32.as_datatype_enum)\n",
    "\n",
    "# Save the optimized graph\n",
    "\n",
    "f = tf.gfile.FastGFile(output_optimized_graph_name, \"w\")\n",
    "f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
